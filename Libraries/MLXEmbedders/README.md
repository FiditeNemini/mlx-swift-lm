#  MLXEmbedders

This directory contains ports of popular Encoders / Embedding Models. 

## Usage Example

```swift
    let modelContainer = try await loadModelContainer(configuration: .nomic_text_v1_5)
    let searchInputs = [
        "search_query: Animals in Tropical Climates.",
        "search_document: Elephants",
        "search_document: Horses",
        "search_document: Polar Bears",
    ]

    // Generate embeddings
    let resultEmbeddings = await modelContainer.perform {
        (model: EmbeddingModel, tokenizer: Tokenizer, pooling: Pooling) -> [[Float]] in
        let inputs = searchInputs.map {
            tokenizer.encode(text: $0, addSpecialTokens: true)
        }
        // Pad to longest
        let maxLength = inputs.reduce(into: 16) { acc, elem in
            acc = max(acc, elem.count)
        }

        let padded = stacked(
            inputs.map { elem in
                MLXArray(
                    elem
                        + Array(
                            repeating: tokenizer.eosTokenId ?? 0,
                            count: maxLength - elem.count))
            })
        let mask = (padded .!= tokenizer.eosTokenId ?? 0)
        let tokenTypes = MLXArray.zeros(like: padded)
        let result = pooling(
            model(padded, positionIds: nil, tokenTypeIds: tokenTypes, attentionMask: mask),
            normalize: true, applyLayerNorm: true
        )
        result.eval()
        return result.map { $0.asArray(Float.self) }
    }
```


Ported to swift from [taylorai/mlx_embedding_models](https://github.com/taylorai/mlx_embedding_models/tree/main)[^1]

[^1]: Modified by [CodebyCR](https://github.com/CodebyCR) to match test case.
